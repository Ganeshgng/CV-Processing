{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%config IPCompleter.greedy=True\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data Set\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "K = len(np.unique(y_train)) # Number of Classes\n",
    "\n",
    "Ntr = x_train.shape[0]\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10 # 32x32x3 = height x width x channel\n",
    "\n",
    "# Normalize pixel values: Image data preprocessing\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = tf.reduce_mean(x_train, axis=0) # axis=0: mean of a column; Mean of each pixel\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test =  tf.keras.utils.to_categorical(y_test,  num_classes=K)\n",
    "\n",
    "x_train = tf.reshape(x_train,(Ntr,Din)).astype('float32')\n",
    "x_test = tf.reshape(x_test,(Nte,Din)).astype('float32')\n",
    "\n",
    "std=1e-5 # For random samples from N(\\mu, \\sigma^2), use: sigma * np.random.randn(...) + mu\n",
    "#w1 = std*np.random.randn(Din, K) # Initializing the weight matrix with random weights\n",
    "w1 = std*tf.random.normal([Din,k], 0, 1)\n",
    "b1 = tf.zeros([K]) # Initializing the bias vector\n",
    "\n",
    "# Rearranging train and test samples: (ra=rearranged)\n",
    "x_train_ra = tf.concat((np.ones((x_train.shape[0],1)),x_train), axis=1)\n",
    "x_test_ra  = tf.concat((np.ones((x_test.shape[0],1)),x_test), axis=1)\n",
    "\n",
    "# Rearranging weight matrix and bias matrix into single matrix\n",
    "w1 = tf.concat((b1.reshape(1,K), w1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "iterations = 300  # Gradient descent interations\n",
    "lr = 1.4e-2 # Learninig rate\n",
    "lr_decay= 0.999\n",
    "reg = 5e-6\n",
    "\n",
    "loss_history = [] # Vlaues of loss function at each iteration \n",
    "test_loss = []\n",
    "train_acc_history = [] # Training accuracy\n",
    "val_acc_history = [] # Validation accuracy\n",
    "\n",
    "\n",
    "m = x_train.shape[0]  # Number of training examples\n",
    "m2 = x_test_ra.shape[0]\n",
    "\n",
    "# Running gradient descent number of times speciied in iterations\n",
    "print(\"Running gradient descent...\")\n",
    "\n",
    "for t in range(1,iterations+1):    \n",
    "    # Forward Propagation\n",
    "    hypothesis = x_train_ra.dot(w1)\n",
    "    loss = (1/(2*m))*np.sum(( hypothesis - y_train)**2) + (1/(2*m))*reg*np.sum(w1**2) \n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Backward Propagation\n",
    "    dw1 = (1/m)*(x_train_ra.T.dot(hypothesis - y_train))  + (1/m)*reg*w1 \n",
    "    w1 = w1 - lr*dw1\n",
    "    \n",
    "    \n",
    "    # Training Accuracy and Validation Accuracy\n",
    "    train_acc = getAccuracy(hypothesis, y_train)\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc = getAccuracy(x_test_ra.dot(w1), y_test)\n",
    "    val_acc_history.append(valid_acc)\n",
    "   \n",
    "    # Test Loss    \n",
    "    t_loss = (1/(2*m2))*np.sum(( x_test_ra.dot(w1) - y_test)**2) + (1/(2*m2))*reg*np.sum(w1**2) \n",
    "    test_loss.append(t_loss)\n",
    "    \n",
    "    # Print details for selected iterations\n",
    "    if (t%30==0) or (t==1):\n",
    "        print(\"| Epoch {:03} | Loss {:.4f} | accuracy: {:.4f} | val_loss: {:.4f} | val_accuracy: {:.4f} |\"\\\n",
    "             .format(t, loss, train_acc, t_loss, valid_acc))\n",
    "    \n",
    "    \n",
    "    # Decaying learning rate\n",
    "    lr = lr*lr_decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
