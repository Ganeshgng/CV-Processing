{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due date: Friday, 2 April 2021, 11:59 PM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%config IPCompleter.greedy=True\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(hypothesis):\n",
    "    return 1/(1+ np.exp(-hypothesis))\n",
    "\n",
    "def getAccuracy(predictions,labels):\n",
    "    pred_class = np.argmax(predictions, axis=1)\n",
    "    real_class = np.argmax(labels, axis=1)\n",
    "    valid_pred = [pred_class == real_class]\n",
    "    return 100*np.sum(valid_pred)/len(real_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data Set\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "# y_train contains labels form 0 to 9 corresponding to 10 classes.\n",
    "K = len(np.unique(y_train)) # Number of Classes\n",
    "Din = 3072 # CIFAR10 # 32x32x3 = height x width x channel\n",
    "# Normalize pixel values: Image data preprocessing\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0) # axis=0: mean of a column; Mean of each pixel\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test =  tf.keras.utils.to_categorical(y_test,  num_classes=K)\n",
    "# Reshaping the tensors into 2D arrays\n",
    "x_train = np.reshape(x_train,(Ntr,Din)).astype('float32')\n",
    "x_test = np.reshape(x_test,(Nte,Din)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference1](https://cs231n.github.io/linear-classify/)\n",
    "\n",
    "A part of the code for a linear classifier for CIFAR10 given in listing 1. For our linear classifier, the score function is f (x) = Wx + b, and the loss function is the mean sum of squared errors function. [3 marks]\n",
    "1. Implement gradient descent and run for 300 epochs.\n",
    "2. Show the weights matrix W as 10 images.\n",
    "3. Report the (initial) learning rate, training and testing loss and accuracies.\n",
    "\n",
    "(Hint: If your loss explodes, reduce the leaning rate.)\n",
    "* [np.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html), [np.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html), [tf.keras.utils.to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical), [np.random.randn](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html), [numpy.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "std=1e-5 \n",
    "w1 = std*np.random.randn(Din, K) # Initializing the weight matrix with random weights\n",
    "b1 = np.zeros(K) # Initializing the bias vector\n",
    "# Rearranging train and test samples: (ra=rearranged)\n",
    "x_train_ra = np.concatenate((np.ones((x_train.shape[0],1)),x_train), axis=1)\n",
    "x_test_ra  = np.concatenate((np.ones((x_test.shape[0],1)),x_test), axis=1)\n",
    "# Rearranging weight matrix and bias matrix into single matrix\n",
    "w1 = np.concatenate((b1.reshape(1,K), w1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = x_train.shape[0]  # Number of training examples\n",
    "for t in range(1,iterations+1):    \n",
    "    # Forward Propagation\n",
    "    hypothesis = x_train_ra.dot(w1)\n",
    "    loss = (1/(2*m))*np.sum(( hypothesis - y_train)**2) + (1/(2*m))*reg*np.sum(w1**2)   \n",
    "    # Backward Propagation\n",
    "    dw1 = (1/m)*(x_train_ra.T.dot(hypothesis - y_train))  + (1/m)*reg*w1 \n",
    "    w1 = w1 - lr*dw1    \n",
    "    # Training Accuracy and Validation Accuracy\n",
    "    train_acc = getAccuracy(hypothesis, y_train)\n",
    "    valid_acc = getAccuracy(x_test_ra.dot(w1), y_test)\n",
    "    # Decaying learning rate\n",
    "    lr_hitory.append(lr)\n",
    "    lr = lr*lr_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "Code a two-layer fully connected network with H = 200 hidden nodes. Choose the sigmoid function as the activation function for the hidden nodes. The output layer has no activation function. [3 marks]\n",
    "\n",
    "1. Implement gradient descent and run for 300 epochs.\n",
    "2. Report the (initial) learning rate, training and testing loss and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 200 # No of hidden nodes\n",
    "std=1e-5 \n",
    "# Hidden Layer \n",
    "w1 = std*np.random.randn(Din, H) # Initializing the weight matrix with random weights\n",
    "b1 = np.zeros(H) # Initializing the bias vector\n",
    "# Last Layer\n",
    "w2 = std*np.random.randn(H, K) # Initializing the weight matrix with random weights\n",
    "b2 = np.zeros(K) # Initializing the bias vector\n",
    "# Rearranging train and test samples: (ra=rearranged)\n",
    "x_train_ra = np.concatenate((np.ones((x_train.shape[0],1)),x_train), axis=1)\n",
    "x_test_ra  = np.concatenate((np.ones((x_test.shape[0],1)),x_test), axis=1)\n",
    "# Rearranging weight matrices and bias vectors into single matrices\n",
    "w1 = np.concatenate((b1.reshape(1,H), w1), axis=0)\n",
    "w2 = np.concatenate((b2.reshape(1,K), w2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1,iterations+1):\n",
    "    # Forward Propagation\n",
    "    hypo = sigmoid(x_train_ra.dot(w1)) # Layer 1 with sigmoid activation\n",
    "    hypothesis = np.concatenate((np.ones((hypo.shape[0],1)),hypo), axis=1) # Rearranging for layer 2\n",
    "    predict = hypothesis.dot(w2) # Layer 2     \n",
    "    loss = (1/(2*m))*np.sum(( predict - y_train)**2)\\\n",
    "         + (1/(2*m))*reg*np.sum(w1**2) + (1/(2*m))*reg*np.sum(w2**2)\n",
    "    # Back Propagation partial dertivatives of Loss function\n",
    "    dpredict =  (1/m)*(predict - y_train)\n",
    "    dw2 = hypothesis.T.dot(dpredict) + (1/m)*reg*w2\n",
    "    dh = dpredict.dot(w2[1:,].T) # Removing bias vector w2(201x10)--> 200x10\n",
    "    dhdxw1 = hypo*(1 - hypo) #using hypothesis 50000*200 the one before rearranging.\n",
    "    dw1 = x_train_ra.T.dot(dh*dhdxw1) + (1/m)*reg*w1    \n",
    "    # Gradient Descent\n",
    "    w1 = w1 - lr*dw1\n",
    "    w2 = w2 - lr*dw2\n",
    "     # Decaying learning rate\n",
    "    lr_hitory.append(lr)\n",
    "    lr = lr*lr_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "Modify the code in item 2 to carry out stochastic gradient descent with a batch size of 500. [2 marks]\n",
    "1. Report training and testing loss and accuracies.\n",
    "2. Compare results with item2 (justify).\n",
    "\n",
    "[Reference](https://realpython.com/gradient-descent-algorithm-python/#minibatches-in-stochastic-gradient-descent)\n",
    "\n",
    "* Stochastic gradient descent randomly divides the set of observations into minibatches.\n",
    "* For each minibatch, the gradient is computed and the vector is moved.\n",
    "* Once all minibatches are used, you say that the iteration, or epoch, is finished and start the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500 # define the batch size\n",
    "seed = 0; rng = np.random.default_rng(seed=seed)\n",
    "for t in range(1,iterations+1):\n",
    "    indices = np.arange(Ntr) #Number of training samples\n",
    "    rng.shuffle(indices)\n",
    "    x_train_3 = x_train_ra[indices]\n",
    "    y_train_3 = y_train[indices]\n",
    "    batch_loss = 0 # Loss for each batch\n",
    "    for start in range(0,Ntr,batch_size):\n",
    "        stop = start + batch_size\n",
    "        # Forward Propagation\n",
    "        hypo = sigmoid(x_train_3[start:stop].dot(w1)) # Layer 1 with sigmoid activation\n",
    "        hypothesis = np.concatenate((np.ones((hypo.shape[0],1)),hypo), axis=1) # Rearranging for layer 2\n",
    "        predict = hypothesis.dot(w2) # Layer 2 \n",
    "        minibatch_loss = (1/(2*m))*np.sum(( predict - y_train_3[start:stop])**2)\\\n",
    "             + (1/(2*m))*reg*np.sum(w1**2) + (1/(2*m))*reg*np.sum(w2**2)\n",
    "        batch_loss+= minibatch_loss\n",
    "        # Back Propagation partial dertivatives of Loss function\n",
    "        dpredict =  (1/m)*(predict - y_train_3[start:stop])\n",
    "        dw2 = hypothesis.T.dot(dpredict) + (1/m)*reg*w2\n",
    "        dh = dpredict.dot(w2[1:,].T) # Removing bias vector w2(201x10)--> 200x10\n",
    "        dhdxw1 = hypo*(1 - hypo) #using hypothesis 50000*200, the one before rearranging.\n",
    "        dw1 = x_train_3[start:stop].T.dot(dh*dhdxw1) + (1/m)*reg*w1\n",
    "        # Gradient Descent\n",
    "        w1 = w1 - lr*dw1\n",
    "        w2 = w2 - lr*dw2    \n",
    "    # Decaying learning rate\n",
    "    lr_hitory.append(lr)\n",
    "    lr = lr*lr_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 \n",
    "Construct a CNN using Keras.models.Sequential (with the following configuration: C32, C64, C64, F64, F10. All three convolutions layers are 3x3. Max pooling (2x2) follows each convolution layer. Use SDG (with momentum) with a batch size of 50 and CategoricalCrossentropy as the loss. [2\n",
    "marks]\n",
    "1. How many learnable parameters are there in this network?\n",
    "2. Report the parameters such as the learning rate and momentum.\n",
    "3. Report training and testing loss and accuracies.\n",
    "\n",
    "[Code Reference](https://www.tensorflow.org/tutorials/images/cnn),\n",
    "[model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit),\n",
    "[CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy),\n",
    "[sgd](https://keras.io/api/optimizers/sgd/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "K = len(np.unique(y_train)) # Number of Classes\n",
    "# Normalize pixel values: Image data preprocessing\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0) # axis=0: mean of a column; Mean of each pixel\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "# Declaring the CNN\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), name='C32'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', name='C64_1')) # 64, 3x3 convolutions\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', name='C64_2')) # 64, 3x3 convolutions\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten()) # Make the (None, 2, 2, 64) tensor flat\n",
    "model.add(layers.Dense(64, activation='relu', name='F64')) # Dense Layer 1\n",
    "model.add(layers.Dense(10, name='F10')) # Because CIFAR has 10 output classes\n",
    "model.summary() # Complete architecture of the model\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1.4e-2, momentum=0.9),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=50, epochs=10, \n",
    "                    validation_data=(x_test, y_test))\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
