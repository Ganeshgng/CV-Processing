{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due date: Friday, 2 April 2021, 11:59 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%config IPCompleter.greedy=True\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveto(filename):\n",
    "    plt.savefig('LaTeX Report/figures/'+ filename)\n",
    "\n",
    "def saveimg(filename, image):\n",
    "    cv.imwrite('LaTeX Report/figures/'+ filename,image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "A part of the code for a linear classifier for CIFAR10 given in listing 1. For our linear classifier, the score function is f (x) = Wx + b, and the loss function is the mean sum of squared errors function. [3 marks]\n",
    "1. Implement gradient descent and run for 300 epochs.\n",
    "2. Show the weights matrix W as 10 images.\n",
    "3. Report the (initial) learning rate, training and testing loss and accuracies.\n",
    "\n",
    "(Hint: If your loss explodes, reduce the leaning rate.)\n",
    "* [np.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html), [np.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html), [tf.keras.utils.to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical), [np.random.randn](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (50000, 32, 32, 3)\n",
      "y_train:  (50000, 1)\n",
      "Number of training samples: 50000\n",
      "Number of test samples:  10000\n",
      "y_train:  (50000, 10)\n",
      "x_train:  (50000, 3072)  x_test:  (10000, 3072)\n",
      "w1: (3072, 10)\n",
      "b1: (10,)\n",
      "(50000, 3073)\n",
      "(3073, 10)\n"
     ]
    }
   ],
   "source": [
    "# Loading the Data Set\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print('x_train: ', x_train.shape); print('y_train: ', y_train.shape)\n",
    "#print(y_train[0:10])\n",
    "\n",
    "# y_train contains labels form 0 to 9 corresponding to 10 classes.\n",
    "K = len(np.unique(y_train)) # Number of Classes\n",
    "\n",
    "Ntr = x_train.shape[0]; print('Number of training samples:', Ntr) # Number of training samples 50,000\n",
    "Nte = x_test.shape[0]; print('Number of test samples: ',Nte)      # Number of test samples 10,000\n",
    "Din = 3072 # CIFAR10 # 32x32x3 = height x width x channel\n",
    "\n",
    "# Normalize pixel values: Image data preprocessing\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0) # axis=0: mean of a column; Mean of each pixel\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K); print('y_train: ', y_train.shape); #print(y_train[0:10,:])\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K); #print(y_test[0:10,:])\n",
    "\n",
    "x_train = np.reshape(x_train,(Ntr,Din)).astype('float32');# print(x_train[0:10, 0:20])\n",
    "x_test = np.reshape(x_test,(Nte,Din)).astype('float32')\n",
    "print('x_train: ', x_train.shape, ' x_test: ', x_test.shape)\n",
    "\n",
    "std=1e-5 # For random samples from N(\\mu, \\sigma^2), use: sigma * np.random.randn(...) + mu\n",
    "w1 = std*np.random.randn(Din, K) # Initializing the weight matrix with random weights\n",
    "b1 = np.zeros(K) # Initializing the bias vector\n",
    "print(\"w1:\", w1.shape);print(\"b1:\", b1.shape)\n",
    "batch_size = Ntr\n",
    "\n",
    "\"\"\"\n",
    "Keep track of two sets of parameters w1 and b1 seperately is not really efficient.\n",
    "This can be eiliminated by combining both of them into one single matrix as follows.\n",
    "Aditionally the bias term '1' must be added infront of each image row, for this to wrok.\n",
    "i.e to enable matrix multiplication.\n",
    "\"\"\"\n",
    "x_train = np.concatenate((np.ones((x_train.shape[0],1)),x_train), axis=1); print(x_train.shape); #print(x_train[0:10,0:10])\n",
    "x_test = np.concatenate((np.ones((x_test.shape[0],1)),x_test), axis=1)\n",
    "\n",
    "w1 = np.concatenate((b1.reshape(1,K), w1), axis=0); print(w1.shape); #print(w1[0:10,:])\n",
    "\n",
    "\n",
    "def cost(X, W):\n",
    "    m = X.shape[0]\n",
    "    cost = (1/m)\n",
    "    return cost\n",
    "    \n",
    "\n",
    "# iterations = 300\n",
    "# lr = \n",
    "# lr_decay=\n",
    "# reg =\n",
    "# loss_history = [] # Vlaues of cost function at each iteration \n",
    "# train_acc_history = []\n",
    "# val_acc_history = []\n",
    "# seed = 0\n",
    "# rng = np.random.default_rng(seed=seed)\n",
    "# for t in range(iterations):\n",
    "#     indices = np.arange(Ntr)\n",
    "#     rng.shuffle(indices)\n",
    "#     Forward pass\n",
    "\n",
    "#     Backward pass\n",
    "    \n",
    "# Printing accuracies and displaying w as images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "Code a two-layer fully connected network with H = 200 hidden nodes. Choose the sigmoid function as the activation function for the hidden nodes. The output layer has no activation function. [3 marks]\n",
    "\n",
    "1. Implement gradient descent and run for 300 epochs.\n",
    "2. Report the (initial) learning rate, training and testing loss and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "Modify the code in item 2 to carry out stochastic gradient descent with a batch size of 500. [2 marks]\n",
    "1. Report training and testing loss and accuracies.\n",
    "2. Compare results with item2 (justify)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 \n",
    "Construct a CNN using Keras.models.Sequential (with the following configuration: C32, C64, C64, F64, F10. All three convolutions layers are 3x3. Max pooling (2x2) follows each convolution layer. Use SDG (with momentum) with a batch size of 50 and CategoricalCrossentropy as the loss. [2\n",
    "marks]\n",
    "1. How many learnable parameters are there in this network?\n",
    "2. Report the parameters such as the learning rate and momentum.\n",
    "3. Report training and testing loss and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
